{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning - The most powerful and exciting subset of ML\n",
    "### An Artificial Neural Networks with many layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks - Modeling cognitive behaviour of human mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement - We are approached by a MNC Bank who wants to know why people are leaving their bank , the bank has collected a sample of 10k customers and got all their basic details as well as tracked them from 6 months and checked whether they left the bank or all still with the bank.\n",
    "### Goal - To find out the primary features due to which people are leaving the bank.\n",
    "### Problem - Classification Problem\n",
    "### Solution - Through Deep Learning(ANN's) Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "#print(X[0])\n",
    "#print(type(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical data(As ML model work with numbers and not text)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1]) #As there are more than 2 countries(to avoid partiality)\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling , as numbers are of large size increasing performance of the algorithm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1]) #For gender (OHEncoder not required)\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2]) #For countries\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1]) #As there are more than 2 countries(to avoid partiality)\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2) Building the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Importing Keras library and required packages\n",
    "import keras #A library through which we can create neural networks in a few lines of codes\n",
    "from keras.models import Sequential #Used to initialize our artificial neural network\n",
    "from keras.layers import Dense #Used to create layers in our artificial neural network\n",
    "from keras.layers import Dropout #Used to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialzing the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Adding the input layer and first hidden layer\n",
    "#output_dim -  Add a hidden layer with 6 no. nodes(Iip - input(11) + output(1) nodes / 2 = 6 nodes ); init is weights close to 0 ; activ = rectifier; input_dim = no.of independent variables\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "classifier.add(Dropout(p = 0.1)) #0.1 means randomly drop 10% of the neurons to avoid overfitting and encourage learning instead of mugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Adding the second and last hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu',))\n",
    "classifier.add(Dropout(p = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 0.4872 - acc: 0.7960\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 3s 331us/step - loss: 0.4344 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 3s 327us/step - loss: 0.4313 - acc: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 3s 323us/step - loss: 0.4254 - acc: 0.7984\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 3s 320us/step - loss: 0.4237 - acc: 0.8187\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 0.4259 - acc: 0.8260\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.4282 - acc: 0.8280\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.4261 - acc: 0.8275\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.4225 - acc: 0.8267\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 0.4225 - acc: 0.8281\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 0.4255 - acc: 0.8312\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.4231 - acc: 0.8296\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 0.4213 - acc: 0.8310\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 0.4216 - acc: 0.8316\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 296us/step - loss: 0.4237 - acc: 0.8292\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 3s 340us/step - loss: 0.4220 - acc: 0.8295\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 3s 321us/step - loss: 0.4191 - acc: 0.8316\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 3s 332us/step - loss: 0.4221 - acc: 0.8289\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 0.4222 - acc: 0.8304\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 3s 348us/step - loss: 0.4217 - acc: 0.8325\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4245 - acc: 0.8306\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 3s 319us/step - loss: 0.4239 - acc: 0.8317\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 3s 343us/step - loss: 0.4219 - acc: 0.8327\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 2s 307us/step - loss: 0.4234 - acc: 0.8304\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 3s 386us/step - loss: 0.4236 - acc: 0.8327\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 3s 340us/step - loss: 0.4225 - acc: 0.8287\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 3s 328us/step - loss: 0.4232 - acc: 0.8291\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.4256 - acc: 0.8297\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.4223 - acc: 0.8312\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.4198 - acc: 0.8311\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.4234 - acc: 0.8331\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.4233 - acc: 0.8356\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 308us/step - loss: 0.4248 - acc: 0.8315\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.4223 - acc: 0.8296\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.4205 - acc: 0.8290\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 0.4224 - acc: 0.8312\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 4s 486us/step - loss: 0.4232 - acc: 0.8306\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 5s 600us/step - loss: 0.4247 - acc: 0.8312\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 4s 501us/step - loss: 0.4208 - acc: 0.8299\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 3s 434us/step - loss: 0.4211 - acc: 0.8305\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 3s 323us/step - loss: 0.4235 - acc: 0.8324\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 2s 305us/step - loss: 0.4232 - acc: 0.8316\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 0.4225 - acc: 0.8306\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.4249 - acc: 0.8310\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.4188 - acc: 0.8321\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 0.4238 - acc: 0.8322\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.4233 - acc: 0.8310\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.4180 - acc: 0.8324\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.4202 - acc: 0.8332\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.4207 - acc: 0.8336\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 0.4211 - acc: 0.8337\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 255us/step - loss: 0.4230 - acc: 0.8316\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.4223 - acc: 0.8314\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 0.4212 - acc: 0.8329\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.4207 - acc: 0.8337\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.4173 - acc: 0.8320\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.4214 - acc: 0.8336\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 0.4200 - acc: 0.8334\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.4230 - acc: 0.8312\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.4208 - acc: 0.8319\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.4224 - acc: 0.8322\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 0.4206 - acc: 0.8311\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 3s 347us/step - loss: 0.4216 - acc: 0.8329\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 3s 364us/step - loss: 0.4181 - acc: 0.8329\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 4s 466us/step - loss: 0.4180 - acc: 0.8327\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 4s 559us/step - loss: 0.4254 - acc: 0.8315\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 4s 532us/step - loss: 0.4208 - acc: 0.8329\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.4206 - acc: 0.8325\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.4205 - acc: 0.8317\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 0.4217 - acc: 0.8337\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.4203 - acc: 0.8321\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.4208 - acc: 0.8336\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4189 - acc: 0.8339\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4213 - acc: 0.8331\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.4213 - acc: 0.8337\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.4184 - acc: 0.8309\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.4184 - acc: 0.8337\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.4216 - acc: 0.8327\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.4193 - acc: 0.8321\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.4226 - acc: 0.8334\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.4218 - acc: 0.8315\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.4209 - acc: 0.8322\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4200 - acc: 0.8337\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.4214 - acc: 0.8324\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.4199 - acc: 0.8340\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.4227 - acc: 0.8319\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.4196 - acc: 0.8319\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.4190 - acc: 0.8334\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 3s 356us/step - loss: 0.4167 - acc: 0.8339\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 3s 342us/step - loss: 0.4192 - acc: 0.8355\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 285us/step - loss: 0.4185 - acc: 0.8332\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.4176 - acc: 0.8329\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.4186 - acc: 0.8345\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 0.4207 - acc: 0.8335\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 0.4183 - acc: 0.8331\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.4174 - acc: 0.8357\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.4177 - acc: 0.8347\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 0.4181 - acc: 0.8345\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.4179 - acc: 0.8330\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.4146 - acc: 0.8309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f026eb7aa90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the ANN to the training set\n",
    "#Batch size - no.of obsv after which you update the weights\n",
    "#epochs - when whole training set is passed through the ANN and back propagated. (1 iteration of the ANN) and then through cost function and back proprogration weights are updated and 2nd iteration(epoch) begins.\n",
    "classifier.fit(X_train,y_train,batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our ANN model is returning us a probability of 85.99% on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 3 - Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "#y_pred will contain all probability that the customer will leave the bank\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.839"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1545 + 133\n",
    "1678 / 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4356c19c44e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Making the Confusion Matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1678 accurate predictions and 322 incorrect predictions; accuracy = no.of correct/ total pred = 83.9 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the bank can create a ranking based on the probability of customers at the highest risk of leaving the bank, from top to bottom and make strategy to retain them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Predicting output of a single observation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use our ANN model to predict if the customer with the following informations will leave the bank: \n",
    "\n",
    "Geography: France\n",
    "Credit Score: 600\n",
    "Gender: Male\n",
    "Age: 40 years old\n",
    "Tenure: 3 years\n",
    "Balance: $60000\n",
    "Number of Products: 2\n",
    "Does this customer have a credit card ? Yes\n",
    "Is this customer an Active Member: Yes\n",
    "Estimated Salary: $50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will manually have to check for one hot encoding for categorical variables from X and dataset matrix\n",
    "#### [0,0] - Dummy encoding for country 'France'\n",
    "#### [0] - Female , hence [1] is male written after 600\n",
    "#### Feature Scaling our data will transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Probability that customer will leave the bank = 7.4 %\n",
      "Will the Customer leave the bank ? A] => [[False]]\n"
     ]
    }
   ],
   "source": [
    "new_pred = classifier.predict(sc.transform(np.array([[0, 0 , 600, 1, 40, 3, 60000, 2, 1, 1, 50000]]))) #As ML models take 2d arrays as input\n",
    "print('The Probability that customer will leave the bank = {} %'.format(round(float(new_pred*100),2)))\n",
    "new_pred = (new_pred > 0.5)\n",
    "print('Will the Customer leave the bank ? A] => {}'.format(str(new_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Evaluating, Improving and Tuning our ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)Evaluating our ANN (Using k-fold cross validation)\n",
    "#### (* Execute the data preprocessing step prior to k-fold model analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our ANN model is giving us an accuracy of 84.0 %\n",
      "Variance in our ANN model = 0.0 \n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs = -1) #cv = no.of k-folds,n_jobs=-1 means using all CPU's\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "print('Our ANN model is giving us an accuracy of {} %'.format(round(mean*100),3))\n",
    "print('Variance in our ANN model = {} '.format(round(variance),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our ANN model is giving us an Relevant accuracy of 84.0 % and a variance of 0.012539935392675127 on the test set\n",
    "### Challenge - Improving our model's accuracy to 86%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)Improving our ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropout regularization to reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Tuning our ANN (Finding the Optimum Parameters for our model using GridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
